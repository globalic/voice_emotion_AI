{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "# from tensorflow.contrib.keras.applications import MobileNet\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix='emotions_en_de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation(='check_') data loaded, 644 items total \n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "dataset = pickle.load(open(os.path.join('Datasets', prefix+'.pkl'), 'rb'))\n",
    "\n",
    "train_indices = [ i for i,r in enumerate(dataset['rand']) if r<=0.9 ]\n",
    "check_indices = [ i for i,r in enumerate(dataset['rand']) if r>0.9 ]\n",
    "\n",
    "print(\"Training and Validation(='check_') data loaded, %d items total \" % (len(dataset['stamp']),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.array( dataset['stamp'] )[train_indices]\n",
    "y_train = np.array( dataset['label'] )[train_indices]\n",
    "\n",
    "x_test = np.array( dataset['stamp'] )[check_indices]\n",
    "y_test = np.array( dataset['label'] )[check_indices]\n",
    "\n",
    "# x_train = [dataset['stamp']][train_indices]\n",
    "# y_train = [dataset['label']][train_indices]\n",
    "\n",
    "# x_test = [dataset['stamp']][check_indices]\n",
    "# y_test = [dataset['label']][check_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_rgb(im):\n",
    "    # I think this will be slow\n",
    "    print(im.shape)\n",
    "    n, w, h = im.shape\n",
    "    ret = np.empty((n, w, h, 3))\n",
    "    ret[:, :, :, 0] = im\n",
    "    ret[:, :, :, 1] = im\n",
    "    ret[:, :, :, 2] = im\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 224, 224)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(570, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.expand_dims( x_train, -1)\n",
    "x_train = to_rgb(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# x_test = np.expand_dims( x_test, -1)\n",
    "x_test = to_rgb(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[   1.,    1.,    1.],\n",
       "         [   1.,    1.,    1.],\n",
       "         [   1.,    1.,    1.],\n",
       "         ..., \n",
       "         [  42.,   42.,   42.],\n",
       "         [  42.,   42.,   42.],\n",
       "         [  42.,   42.,   42.]],\n",
       "\n",
       "        [[  11.,   11.,   11.],\n",
       "         [  11.,   11.,   11.],\n",
       "         [  11.,   11.,   11.],\n",
       "         ..., \n",
       "         [  70.,   70.,   70.],\n",
       "         [  70.,   70.,   70.],\n",
       "         [  70.,   70.,   70.]],\n",
       "\n",
       "        [[  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         ..., \n",
       "         [  88.,   88.,   88.],\n",
       "         [  88.,   88.,   88.],\n",
       "         [  88.,   88.,   88.]],\n",
       "\n",
       "        ..., \n",
       "        [[  39.,   39.,   39.],\n",
       "         [  39.,   39.,   39.],\n",
       "         [  39.,   39.,   39.],\n",
       "         ..., \n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.]],\n",
       "\n",
       "        [[  23.,   23.,   23.],\n",
       "         [  23.,   23.,   23.],\n",
       "         [  23.,   23.,   23.],\n",
       "         ..., \n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.]],\n",
       "\n",
       "        [[  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.],\n",
       "         ..., \n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.],\n",
       "         [  62.,   62.,   62.]]],\n",
       "\n",
       "\n",
       "       [[[  58.,   58.,   58.],\n",
       "         [  58.,   58.,   58.],\n",
       "         [  58.,   58.,   58.],\n",
       "         ..., \n",
       "         [  58.,   58.,   58.],\n",
       "         [  58.,   58.,   58.],\n",
       "         [  58.,   58.,   58.]],\n",
       "\n",
       "        [[  27.,   27.,   27.],\n",
       "         [  27.,   27.,   27.],\n",
       "         [  27.,   27.,   27.],\n",
       "         ..., \n",
       "         [  56.,   56.,   56.],\n",
       "         [  56.,   56.,   56.],\n",
       "         [  56.,   56.,   56.]],\n",
       "\n",
       "        [[  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.],\n",
       "         ..., \n",
       "         [  55.,   55.,   55.],\n",
       "         [  55.,   55.,   55.],\n",
       "         [  55.,   55.,   55.]],\n",
       "\n",
       "        ..., \n",
       "        [[  60.,   60.,   60.],\n",
       "         [  60.,   60.,   60.],\n",
       "         [  60.,   60.,   60.],\n",
       "         ..., \n",
       "         [  13.,   13.,   13.],\n",
       "         [  13.,   13.,   13.],\n",
       "         [  13.,   13.,   13.]],\n",
       "\n",
       "        [[  64.,   64.,   64.],\n",
       "         [  64.,   64.,   64.],\n",
       "         [  64.,   64.,   64.],\n",
       "         ..., \n",
       "         [  16.,   16.,   16.],\n",
       "         [  16.,   16.,   16.],\n",
       "         [  16.,   16.,   16.]],\n",
       "\n",
       "        [[  72.,   72.,   72.],\n",
       "         [  72.,   72.,   72.],\n",
       "         [  72.,   72.,   72.],\n",
       "         ..., \n",
       "         [  22.,   22.,   22.],\n",
       "         [  22.,   22.,   22.],\n",
       "         [  22.,   22.,   22.]]],\n",
       "\n",
       "\n",
       "       [[[   0.,    0.,    0.],\n",
       "         [   0.,    0.,    0.],\n",
       "         [   0.,    0.,    0.],\n",
       "         ..., \n",
       "         [  51.,   51.,   51.],\n",
       "         [  51.,   51.,   51.],\n",
       "         [  51.,   51.,   51.]],\n",
       "\n",
       "        [[   5.,    5.,    5.],\n",
       "         [   5.,    5.,    5.],\n",
       "         [   5.,    5.,    5.],\n",
       "         ..., \n",
       "         [  50.,   50.,   50.],\n",
       "         [  50.,   50.,   50.],\n",
       "         [  50.,   50.,   50.]],\n",
       "\n",
       "        [[  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.],\n",
       "         ..., \n",
       "         [  75.,   75.,   75.],\n",
       "         [  75.,   75.,   75.],\n",
       "         [  75.,   75.,   75.]],\n",
       "\n",
       "        ..., \n",
       "        [[  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         ..., \n",
       "         [  38.,   38.,   38.],\n",
       "         [  38.,   38.,   38.],\n",
       "         [  38.,   38.,   38.]],\n",
       "\n",
       "        [[  35.,   35.,   35.],\n",
       "         [  35.,   35.,   35.],\n",
       "         [  35.,   35.,   35.],\n",
       "         ..., \n",
       "         [  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.],\n",
       "         [  32.,   32.,   32.]],\n",
       "\n",
       "        [[  34.,   34.,   34.],\n",
       "         [  34.,   34.,   34.],\n",
       "         [  34.,   34.,   34.],\n",
       "         ..., \n",
       "         [  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[  92.,   92.,   92.],\n",
       "         [  92.,   92.,   92.],\n",
       "         [  92.,   92.,   92.],\n",
       "         ..., \n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.]],\n",
       "\n",
       "        [[  77.,   77.,   77.],\n",
       "         [  77.,   77.,   77.],\n",
       "         [  77.,   77.,   77.],\n",
       "         ..., \n",
       "         [  36.,   36.,   36.],\n",
       "         [  36.,   36.,   36.],\n",
       "         [  36.,   36.,   36.]],\n",
       "\n",
       "        [[  90.,   90.,   90.],\n",
       "         [  90.,   90.,   90.],\n",
       "         [  90.,   90.,   90.],\n",
       "         ..., \n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.],\n",
       "         [  33.,   33.,   33.]],\n",
       "\n",
       "        ..., \n",
       "        [[  76.,   76.,   76.],\n",
       "         [  76.,   76.,   76.],\n",
       "         [  76.,   76.,   76.],\n",
       "         ..., \n",
       "         [  35.,   35.,   35.],\n",
       "         [  35.,   35.,   35.],\n",
       "         [  35.,   35.,   35.]],\n",
       "\n",
       "        [[  70.,   70.,   70.],\n",
       "         [  70.,   70.,   70.],\n",
       "         [  70.,   70.,   70.],\n",
       "         ..., \n",
       "         [  28.,   28.,   28.],\n",
       "         [  28.,   28.,   28.],\n",
       "         [  28.,   28.,   28.]],\n",
       "\n",
       "        [[  74.,   74.,   74.],\n",
       "         [  74.,   74.,   74.],\n",
       "         [  74.,   74.,   74.],\n",
       "         ..., \n",
       "         [  24.,   24.,   24.],\n",
       "         [  24.,   24.,   24.],\n",
       "         [  24.,   24.,   24.]]],\n",
       "\n",
       "\n",
       "       [[[  74.,   74.,   74.],\n",
       "         [  74.,   74.,   74.],\n",
       "         [  74.,   74.,   74.],\n",
       "         ..., \n",
       "         [  21.,   21.,   21.],\n",
       "         [  21.,   21.,   21.],\n",
       "         [  21.,   21.,   21.]],\n",
       "\n",
       "        [[  71.,   71.,   71.],\n",
       "         [  71.,   71.,   71.],\n",
       "         [  71.,   71.,   71.],\n",
       "         ..., \n",
       "         [   2.,    2.,    2.],\n",
       "         [   2.,    2.,    2.],\n",
       "         [   2.,    2.,    2.]],\n",
       "\n",
       "        [[  79.,   79.,   79.],\n",
       "         [  79.,   79.,   79.],\n",
       "         [  79.,   79.,   79.],\n",
       "         ..., \n",
       "         [  18.,   18.,   18.],\n",
       "         [  18.,   18.,   18.],\n",
       "         [  18.,   18.,   18.]],\n",
       "\n",
       "        ..., \n",
       "        [[  47.,   47.,   47.],\n",
       "         [  47.,   47.,   47.],\n",
       "         [  47.,   47.,   47.],\n",
       "         ..., \n",
       "         [  22.,   22.,   22.],\n",
       "         [  22.,   22.,   22.],\n",
       "         [  22.,   22.,   22.]],\n",
       "\n",
       "        [[  73.,   73.,   73.],\n",
       "         [  73.,   73.,   73.],\n",
       "         [  73.,   73.,   73.],\n",
       "         ..., \n",
       "         [  26.,   26.,   26.],\n",
       "         [  26.,   26.,   26.],\n",
       "         [  26.,   26.,   26.]],\n",
       "\n",
       "        [[  92.,   92.,   92.],\n",
       "         [  92.,   92.,   92.],\n",
       "         [  92.,   92.,   92.],\n",
       "         ..., \n",
       "         [  14.,   14.,   14.],\n",
       "         [  14.,   14.,   14.],\n",
       "         [  14.,   14.,   14.]]],\n",
       "\n",
       "\n",
       "       [[[  90.,   90.,   90.],\n",
       "         [  90.,   90.,   90.],\n",
       "         [  90.,   90.,   90.],\n",
       "         ..., \n",
       "         [  21.,   21.,   21.],\n",
       "         [  21.,   21.,   21.],\n",
       "         [  21.,   21.,   21.]],\n",
       "\n",
       "        [[ 102.,  102.,  102.],\n",
       "         [ 102.,  102.,  102.],\n",
       "         [ 102.,  102.,  102.],\n",
       "         ..., \n",
       "         [  26.,   26.,   26.],\n",
       "         [  26.,   26.,   26.],\n",
       "         [  26.,   26.,   26.]],\n",
       "\n",
       "        [[  91.,   91.,   91.],\n",
       "         [  91.,   91.,   91.],\n",
       "         [  91.,   91.,   91.],\n",
       "         ..., \n",
       "         [  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.],\n",
       "         [  25.,   25.,   25.]],\n",
       "\n",
       "        ..., \n",
       "        [[  83.,   83.,   83.],\n",
       "         [  83.,   83.,   83.],\n",
       "         [  83.,   83.,   83.],\n",
       "         ..., \n",
       "         [  37.,   37.,   37.],\n",
       "         [  37.,   37.,   37.],\n",
       "         [  37.,   37.,   37.]],\n",
       "\n",
       "        [[ 105.,  105.,  105.],\n",
       "         [ 105.,  105.,  105.],\n",
       "         [ 105.,  105.,  105.],\n",
       "         ..., \n",
       "         [  40.,   40.,   40.],\n",
       "         [  40.,   40.,   40.],\n",
       "         [  40.,   40.,   40.]],\n",
       "\n",
       "        [[  87.,   87.,   87.],\n",
       "         [  87.,   87.,   87.],\n",
       "         [  87.,   87.,   87.],\n",
       "         ..., \n",
       "         [  15.,   15.,   15.],\n",
       "         [  15.,   15.,   15.],\n",
       "         [  15.,   15.,   15.]]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.00392157,  0.00392157,  0.00392157],\n",
       "         [ 0.00392157,  0.00392157,  0.00392157],\n",
       "         [ 0.00392157,  0.00392157,  0.00392157],\n",
       "         ..., \n",
       "         [ 0.16470588,  0.16470588,  0.16470588],\n",
       "         [ 0.16470588,  0.16470588,  0.16470588],\n",
       "         [ 0.16470588,  0.16470588,  0.16470588]],\n",
       "\n",
       "        [[ 0.04313725,  0.04313725,  0.04313725],\n",
       "         [ 0.04313725,  0.04313725,  0.04313725],\n",
       "         [ 0.04313725,  0.04313725,  0.04313725],\n",
       "         ..., \n",
       "         [ 0.2745098 ,  0.2745098 ,  0.2745098 ],\n",
       "         [ 0.2745098 ,  0.2745098 ,  0.2745098 ],\n",
       "         [ 0.2745098 ,  0.2745098 ,  0.2745098 ]],\n",
       "\n",
       "        [[ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         ..., \n",
       "         [ 0.34509804,  0.34509804,  0.34509804],\n",
       "         [ 0.34509804,  0.34509804,  0.34509804],\n",
       "         [ 0.34509804,  0.34509804,  0.34509804]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.15294118,  0.15294118,  0.15294118],\n",
       "         [ 0.15294118,  0.15294118,  0.15294118],\n",
       "         [ 0.15294118,  0.15294118,  0.15294118],\n",
       "         ..., \n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725]],\n",
       "\n",
       "        [[ 0.09019608,  0.09019608,  0.09019608],\n",
       "         [ 0.09019608,  0.09019608,  0.09019608],\n",
       "         [ 0.09019608,  0.09019608,  0.09019608],\n",
       "         ..., \n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725]],\n",
       "\n",
       "        [[ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725],\n",
       "         [ 0.24313725,  0.24313725,  0.24313725]]],\n",
       "\n",
       "\n",
       "       [[[ 0.22745098,  0.22745098,  0.22745098],\n",
       "         [ 0.22745098,  0.22745098,  0.22745098],\n",
       "         [ 0.22745098,  0.22745098,  0.22745098],\n",
       "         ..., \n",
       "         [ 0.22745098,  0.22745098,  0.22745098],\n",
       "         [ 0.22745098,  0.22745098,  0.22745098],\n",
       "         [ 0.22745098,  0.22745098,  0.22745098]],\n",
       "\n",
       "        [[ 0.10588235,  0.10588235,  0.10588235],\n",
       "         [ 0.10588235,  0.10588235,  0.10588235],\n",
       "         [ 0.10588235,  0.10588235,  0.10588235],\n",
       "         ..., \n",
       "         [ 0.21960784,  0.21960784,  0.21960784],\n",
       "         [ 0.21960784,  0.21960784,  0.21960784],\n",
       "         [ 0.21960784,  0.21960784,  0.21960784]],\n",
       "\n",
       "        [[ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         ..., \n",
       "         [ 0.21568627,  0.21568627,  0.21568627],\n",
       "         [ 0.21568627,  0.21568627,  0.21568627],\n",
       "         [ 0.21568627,  0.21568627,  0.21568627]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.23529412,  0.23529412,  0.23529412],\n",
       "         [ 0.23529412,  0.23529412,  0.23529412],\n",
       "         [ 0.23529412,  0.23529412,  0.23529412],\n",
       "         ..., \n",
       "         [ 0.05098039,  0.05098039,  0.05098039],\n",
       "         [ 0.05098039,  0.05098039,  0.05098039],\n",
       "         [ 0.05098039,  0.05098039,  0.05098039]],\n",
       "\n",
       "        [[ 0.25098039,  0.25098039,  0.25098039],\n",
       "         [ 0.25098039,  0.25098039,  0.25098039],\n",
       "         [ 0.25098039,  0.25098039,  0.25098039],\n",
       "         ..., \n",
       "         [ 0.0627451 ,  0.0627451 ,  0.0627451 ],\n",
       "         [ 0.0627451 ,  0.0627451 ,  0.0627451 ],\n",
       "         [ 0.0627451 ,  0.0627451 ,  0.0627451 ]],\n",
       "\n",
       "        [[ 0.28235294,  0.28235294,  0.28235294],\n",
       "         [ 0.28235294,  0.28235294,  0.28235294],\n",
       "         [ 0.28235294,  0.28235294,  0.28235294],\n",
       "         ..., \n",
       "         [ 0.08627451,  0.08627451,  0.08627451],\n",
       "         [ 0.08627451,  0.08627451,  0.08627451],\n",
       "         [ 0.08627451,  0.08627451,  0.08627451]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.2       ,  0.2       ,  0.2       ],\n",
       "         [ 0.2       ,  0.2       ,  0.2       ],\n",
       "         [ 0.2       ,  0.2       ,  0.2       ]],\n",
       "\n",
       "        [[ 0.01960784,  0.01960784,  0.01960784],\n",
       "         [ 0.01960784,  0.01960784,  0.01960784],\n",
       "         [ 0.01960784,  0.01960784,  0.01960784],\n",
       "         ..., \n",
       "         [ 0.19607843,  0.19607843,  0.19607843],\n",
       "         [ 0.19607843,  0.19607843,  0.19607843],\n",
       "         [ 0.19607843,  0.19607843,  0.19607843]],\n",
       "\n",
       "        [[ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         ..., \n",
       "         [ 0.29411765,  0.29411765,  0.29411765],\n",
       "         [ 0.29411765,  0.29411765,  0.29411765],\n",
       "         [ 0.29411765,  0.29411765,  0.29411765]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         ..., \n",
       "         [ 0.14901961,  0.14901961,  0.14901961],\n",
       "         [ 0.14901961,  0.14901961,  0.14901961],\n",
       "         [ 0.14901961,  0.14901961,  0.14901961]],\n",
       "\n",
       "        [[ 0.1372549 ,  0.1372549 ,  0.1372549 ],\n",
       "         [ 0.1372549 ,  0.1372549 ,  0.1372549 ],\n",
       "         [ 0.1372549 ,  0.1372549 ,  0.1372549 ],\n",
       "         ..., \n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ],\n",
       "         [ 0.1254902 ,  0.1254902 ,  0.1254902 ]],\n",
       "\n",
       "        [[ 0.13333333,  0.13333333,  0.13333333],\n",
       "         [ 0.13333333,  0.13333333,  0.13333333],\n",
       "         [ 0.13333333,  0.13333333,  0.13333333],\n",
       "         ..., \n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.36078431,  0.36078431,  0.36078431],\n",
       "         [ 0.36078431,  0.36078431,  0.36078431],\n",
       "         [ 0.36078431,  0.36078431,  0.36078431],\n",
       "         ..., \n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176]],\n",
       "\n",
       "        [[ 0.30196078,  0.30196078,  0.30196078],\n",
       "         [ 0.30196078,  0.30196078,  0.30196078],\n",
       "         [ 0.30196078,  0.30196078,  0.30196078],\n",
       "         ..., \n",
       "         [ 0.14117647,  0.14117647,  0.14117647],\n",
       "         [ 0.14117647,  0.14117647,  0.14117647],\n",
       "         [ 0.14117647,  0.14117647,  0.14117647]],\n",
       "\n",
       "        [[ 0.35294118,  0.35294118,  0.35294118],\n",
       "         [ 0.35294118,  0.35294118,  0.35294118],\n",
       "         [ 0.35294118,  0.35294118,  0.35294118],\n",
       "         ..., \n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176],\n",
       "         [ 0.12941176,  0.12941176,  0.12941176]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.29803922,  0.29803922,  0.29803922],\n",
       "         [ 0.29803922,  0.29803922,  0.29803922],\n",
       "         [ 0.29803922,  0.29803922,  0.29803922],\n",
       "         ..., \n",
       "         [ 0.1372549 ,  0.1372549 ,  0.1372549 ],\n",
       "         [ 0.1372549 ,  0.1372549 ,  0.1372549 ],\n",
       "         [ 0.1372549 ,  0.1372549 ,  0.1372549 ]],\n",
       "\n",
       "        [[ 0.2745098 ,  0.2745098 ,  0.2745098 ],\n",
       "         [ 0.2745098 ,  0.2745098 ,  0.2745098 ],\n",
       "         [ 0.2745098 ,  0.2745098 ,  0.2745098 ],\n",
       "         ..., \n",
       "         [ 0.10980392,  0.10980392,  0.10980392],\n",
       "         [ 0.10980392,  0.10980392,  0.10980392],\n",
       "         [ 0.10980392,  0.10980392,  0.10980392]],\n",
       "\n",
       "        [[ 0.29019608,  0.29019608,  0.29019608],\n",
       "         [ 0.29019608,  0.29019608,  0.29019608],\n",
       "         [ 0.29019608,  0.29019608,  0.29019608],\n",
       "         ..., \n",
       "         [ 0.09411765,  0.09411765,  0.09411765],\n",
       "         [ 0.09411765,  0.09411765,  0.09411765],\n",
       "         [ 0.09411765,  0.09411765,  0.09411765]]],\n",
       "\n",
       "\n",
       "       [[[ 0.29019608,  0.29019608,  0.29019608],\n",
       "         [ 0.29019608,  0.29019608,  0.29019608],\n",
       "         [ 0.29019608,  0.29019608,  0.29019608],\n",
       "         ..., \n",
       "         [ 0.08235294,  0.08235294,  0.08235294],\n",
       "         [ 0.08235294,  0.08235294,  0.08235294],\n",
       "         [ 0.08235294,  0.08235294,  0.08235294]],\n",
       "\n",
       "        [[ 0.27843137,  0.27843137,  0.27843137],\n",
       "         [ 0.27843137,  0.27843137,  0.27843137],\n",
       "         [ 0.27843137,  0.27843137,  0.27843137],\n",
       "         ..., \n",
       "         [ 0.00784314,  0.00784314,  0.00784314],\n",
       "         [ 0.00784314,  0.00784314,  0.00784314],\n",
       "         [ 0.00784314,  0.00784314,  0.00784314]],\n",
       "\n",
       "        [[ 0.30980392,  0.30980392,  0.30980392],\n",
       "         [ 0.30980392,  0.30980392,  0.30980392],\n",
       "         [ 0.30980392,  0.30980392,  0.30980392],\n",
       "         ..., \n",
       "         [ 0.07058824,  0.07058824,  0.07058824],\n",
       "         [ 0.07058824,  0.07058824,  0.07058824],\n",
       "         [ 0.07058824,  0.07058824,  0.07058824]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.18431373,  0.18431373,  0.18431373],\n",
       "         [ 0.18431373,  0.18431373,  0.18431373],\n",
       "         [ 0.18431373,  0.18431373,  0.18431373],\n",
       "         ..., \n",
       "         [ 0.08627451,  0.08627451,  0.08627451],\n",
       "         [ 0.08627451,  0.08627451,  0.08627451],\n",
       "         [ 0.08627451,  0.08627451,  0.08627451]],\n",
       "\n",
       "        [[ 0.28627451,  0.28627451,  0.28627451],\n",
       "         [ 0.28627451,  0.28627451,  0.28627451],\n",
       "         [ 0.28627451,  0.28627451,  0.28627451],\n",
       "         ..., \n",
       "         [ 0.10196078,  0.10196078,  0.10196078],\n",
       "         [ 0.10196078,  0.10196078,  0.10196078],\n",
       "         [ 0.10196078,  0.10196078,  0.10196078]],\n",
       "\n",
       "        [[ 0.36078431,  0.36078431,  0.36078431],\n",
       "         [ 0.36078431,  0.36078431,  0.36078431],\n",
       "         [ 0.36078431,  0.36078431,  0.36078431],\n",
       "         ..., \n",
       "         [ 0.05490196,  0.05490196,  0.05490196],\n",
       "         [ 0.05490196,  0.05490196,  0.05490196],\n",
       "         [ 0.05490196,  0.05490196,  0.05490196]]],\n",
       "\n",
       "\n",
       "       [[[ 0.35294118,  0.35294118,  0.35294118],\n",
       "         [ 0.35294118,  0.35294118,  0.35294118],\n",
       "         [ 0.35294118,  0.35294118,  0.35294118],\n",
       "         ..., \n",
       "         [ 0.08235294,  0.08235294,  0.08235294],\n",
       "         [ 0.08235294,  0.08235294,  0.08235294],\n",
       "         [ 0.08235294,  0.08235294,  0.08235294]],\n",
       "\n",
       "        [[ 0.4       ,  0.4       ,  0.4       ],\n",
       "         [ 0.4       ,  0.4       ,  0.4       ],\n",
       "         [ 0.4       ,  0.4       ,  0.4       ],\n",
       "         ..., \n",
       "         [ 0.10196078,  0.10196078,  0.10196078],\n",
       "         [ 0.10196078,  0.10196078,  0.10196078],\n",
       "         [ 0.10196078,  0.10196078,  0.10196078]],\n",
       "\n",
       "        [[ 0.35686275,  0.35686275,  0.35686275],\n",
       "         [ 0.35686275,  0.35686275,  0.35686275],\n",
       "         [ 0.35686275,  0.35686275,  0.35686275],\n",
       "         ..., \n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922],\n",
       "         [ 0.09803922,  0.09803922,  0.09803922]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.3254902 ,  0.3254902 ,  0.3254902 ],\n",
       "         [ 0.3254902 ,  0.3254902 ,  0.3254902 ],\n",
       "         [ 0.3254902 ,  0.3254902 ,  0.3254902 ],\n",
       "         ..., \n",
       "         [ 0.14509804,  0.14509804,  0.14509804],\n",
       "         [ 0.14509804,  0.14509804,  0.14509804],\n",
       "         [ 0.14509804,  0.14509804,  0.14509804]],\n",
       "\n",
       "        [[ 0.41176471,  0.41176471,  0.41176471],\n",
       "         [ 0.41176471,  0.41176471,  0.41176471],\n",
       "         [ 0.41176471,  0.41176471,  0.41176471],\n",
       "         ..., \n",
       "         [ 0.15686275,  0.15686275,  0.15686275],\n",
       "         [ 0.15686275,  0.15686275,  0.15686275],\n",
       "         [ 0.15686275,  0.15686275,  0.15686275]],\n",
       "\n",
       "        [[ 0.34117647,  0.34117647,  0.34117647],\n",
       "         [ 0.34117647,  0.34117647,  0.34117647],\n",
       "         [ 0.34117647,  0.34117647,  0.34117647],\n",
       "         ..., \n",
       "         [ 0.05882353,  0.05882353,  0.05882353],\n",
       "         [ 0.05882353,  0.05882353,  0.05882353],\n",
       "         [ 0.05882353,  0.05882353,  0.05882353]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "training_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobilenet_base_model = MobileNet(\n",
    "    input_shape=(224, 224, 3),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "#     pooling=None,\n",
    "    pooling='avg',\n",
    "    classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobilenet_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_feature_train = mobilenet_base_model.predict(x_train)\n",
    "np.save('bottleneck_features_train.npy', bottleneck_feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 224, 224, 3)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_feature_test = mobilenet_base_model.predict(x_test)\n",
    "np.save('bottleneck_feature_test.npy', bottleneck_feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_train_data = np.load('bottleneck_features_train.npy')\n",
    "bottleneck_validation_data = np.load('bottleneck_feature_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New top layers after mobilenet_base_model\n",
    "# x = Flatten()(mobilenet_base_model.output)\n",
    "Inp = Input(shape=(1024,))\n",
    "# x = Dropout(0.2)(mobilenet_base_model.output)\n",
    "x = Dropout(0.2)(Inp)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, activation='relu', name = \"Dense_1\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "predictions = Dense(num_classes, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 1,059,842\n",
      "Trainable params: 1,055,746\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "top_model = Model(input=Inp, output=predictions)\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 1024)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 1024)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 570 samples, validate on 74 samples\n",
      "Epoch 1/100\n",
      "570/570 [==============================] - 2s - loss: 0.9850 - acc: 0.5982 - val_loss: 0.6644 - val_acc: 0.6216\n",
      "Epoch 2/100\n",
      "570/570 [==============================] - 0s - loss: 0.6026 - acc: 0.7632 - val_loss: 0.7267 - val_acc: 0.6486\n",
      "Epoch 3/100\n",
      "570/570 [==============================] - 0s - loss: 0.4809 - acc: 0.7947 - val_loss: 0.7053 - val_acc: 0.6216\n",
      "Epoch 4/100\n",
      "570/570 [==============================] - 0s - loss: 0.4731 - acc: 0.8123 - val_loss: 0.6591 - val_acc: 0.6622\n",
      "Epoch 5/100\n",
      "570/570 [==============================] - 0s - loss: 0.3685 - acc: 0.8351 - val_loss: 0.6288 - val_acc: 0.6892\n",
      "Epoch 6/100\n",
      "570/570 [==============================] - 0s - loss: 0.4301 - acc: 0.8158 - val_loss: 0.6553 - val_acc: 0.7162\n",
      "Epoch 7/100\n",
      "570/570 [==============================] - 0s - loss: 0.3581 - acc: 0.8544 - val_loss: 0.6796 - val_acc: 0.7297\n",
      "Epoch 8/100\n",
      "570/570 [==============================] - 0s - loss: 0.3046 - acc: 0.8772 - val_loss: 0.6871 - val_acc: 0.7162\n",
      "Epoch 9/100\n",
      "570/570 [==============================] - 0s - loss: 0.2609 - acc: 0.8807 - val_loss: 0.6507 - val_acc: 0.7162\n",
      "Epoch 10/100\n",
      "570/570 [==============================] - 0s - loss: 0.2529 - acc: 0.8930 - val_loss: 0.6191 - val_acc: 0.7297\n",
      "Epoch 11/100\n",
      "570/570 [==============================] - 0s - loss: 0.2453 - acc: 0.8877 - val_loss: 0.6738 - val_acc: 0.7432\n",
      "Epoch 12/100\n",
      "570/570 [==============================] - 0s - loss: 0.2155 - acc: 0.9105 - val_loss: 0.7011 - val_acc: 0.7568\n",
      "Epoch 13/100\n",
      "570/570 [==============================] - 0s - loss: 0.2127 - acc: 0.9175 - val_loss: 0.6669 - val_acc: 0.7432\n",
      "Epoch 14/100\n",
      "570/570 [==============================] - 0s - loss: 0.1723 - acc: 0.9298 - val_loss: 0.8550 - val_acc: 0.7027\n",
      "Epoch 15/100\n",
      "570/570 [==============================] - 0s - loss: 0.2501 - acc: 0.8947 - val_loss: 0.7945 - val_acc: 0.7162\n",
      "Epoch 16/100\n",
      "570/570 [==============================] - 0s - loss: 0.1940 - acc: 0.9228 - val_loss: 0.7842 - val_acc: 0.7162\n",
      "Epoch 17/100\n",
      "570/570 [==============================] - 0s - loss: 0.1709 - acc: 0.9298 - val_loss: 0.8758 - val_acc: 0.7027\n",
      "Epoch 18/100\n",
      "570/570 [==============================] - 0s - loss: 0.2310 - acc: 0.9263 - val_loss: 0.9989 - val_acc: 0.7297\n",
      "Epoch 19/100\n",
      "570/570 [==============================] - 0s - loss: 0.1633 - acc: 0.9404 - val_loss: 0.9145 - val_acc: 0.7162\n",
      "Epoch 20/100\n",
      "570/570 [==============================] - 0s - loss: 0.1533 - acc: 0.9456 - val_loss: 0.8509 - val_acc: 0.7297\n",
      "Epoch 21/100\n",
      "570/570 [==============================] - 0s - loss: 0.1454 - acc: 0.9421 - val_loss: 0.7815 - val_acc: 0.7027\n",
      "Epoch 22/100\n",
      "570/570 [==============================] - 0s - loss: 0.1384 - acc: 0.9509 - val_loss: 1.0081 - val_acc: 0.7027\n",
      "Epoch 23/100\n",
      "570/570 [==============================] - 0s - loss: 0.1413 - acc: 0.9526 - val_loss: 1.0019 - val_acc: 0.7027\n",
      "Epoch 24/100\n",
      "570/570 [==============================] - 0s - loss: 0.1126 - acc: 0.9561 - val_loss: 0.9306 - val_acc: 0.7162\n",
      "Epoch 25/100\n",
      "570/570 [==============================] - 0s - loss: 0.1109 - acc: 0.9579 - val_loss: 0.8676 - val_acc: 0.7297\n",
      "Epoch 26/100\n",
      "570/570 [==============================] - 0s - loss: 0.1191 - acc: 0.9474 - val_loss: 0.9781 - val_acc: 0.7568\n",
      "Epoch 27/100\n",
      "570/570 [==============================] - 0s - loss: 0.1364 - acc: 0.9421 - val_loss: 0.8833 - val_acc: 0.7432\n",
      "Epoch 28/100\n",
      "570/570 [==============================] - 0s - loss: 0.1097 - acc: 0.9526 - val_loss: 0.9178 - val_acc: 0.7568\n",
      "Epoch 29/100\n",
      "570/570 [==============================] - 0s - loss: 0.0840 - acc: 0.9684 - val_loss: 0.9824 - val_acc: 0.6892\n",
      "Epoch 30/100\n",
      "570/570 [==============================] - 0s - loss: 0.1122 - acc: 0.9456 - val_loss: 1.1157 - val_acc: 0.7027\n",
      "Epoch 31/100\n",
      "570/570 [==============================] - 0s - loss: 0.0661 - acc: 0.9719 - val_loss: 1.0788 - val_acc: 0.7027\n",
      "Epoch 32/100\n",
      "570/570 [==============================] - 0s - loss: 0.0951 - acc: 0.9596 - val_loss: 1.0551 - val_acc: 0.7162\n",
      "Epoch 33/100\n",
      "570/570 [==============================] - 0s - loss: 0.0673 - acc: 0.9772 - val_loss: 1.1505 - val_acc: 0.7297\n",
      "Epoch 34/100\n",
      "570/570 [==============================] - 0s - loss: 0.1055 - acc: 0.9579 - val_loss: 1.1477 - val_acc: 0.7297\n",
      "Epoch 35/100\n",
      "570/570 [==============================] - 0s - loss: 0.1013 - acc: 0.9596 - val_loss: 1.0736 - val_acc: 0.7432\n",
      "Epoch 36/100\n",
      "570/570 [==============================] - 0s - loss: 0.1054 - acc: 0.9596 - val_loss: 1.1367 - val_acc: 0.7297\n",
      "Epoch 37/100\n",
      "570/570 [==============================] - 0s - loss: 0.0760 - acc: 0.9772 - val_loss: 1.1797 - val_acc: 0.7297\n",
      "Epoch 38/100\n",
      "570/570 [==============================] - 0s - loss: 0.0924 - acc: 0.9614 - val_loss: 1.0826 - val_acc: 0.7162\n",
      "Epoch 39/100\n",
      "570/570 [==============================] - 0s - loss: 0.0804 - acc: 0.9632 - val_loss: 1.0398 - val_acc: 0.7297\n",
      "Epoch 40/100\n",
      "570/570 [==============================] - 0s - loss: 0.0807 - acc: 0.9719 - val_loss: 1.0823 - val_acc: 0.7297\n",
      "Epoch 41/100\n",
      "570/570 [==============================] - 0s - loss: 0.1303 - acc: 0.9509 - val_loss: 1.1375 - val_acc: 0.6757\n",
      "Epoch 42/100\n",
      "570/570 [==============================] - 0s - loss: 0.1268 - acc: 0.9421 - val_loss: 1.0129 - val_acc: 0.6622\n",
      "Epoch 43/100\n",
      "570/570 [==============================] - 0s - loss: 0.1000 - acc: 0.9649 - val_loss: 1.2359 - val_acc: 0.7162\n",
      "Epoch 44/100\n",
      "570/570 [==============================] - 0s - loss: 0.0749 - acc: 0.9737 - val_loss: 1.0888 - val_acc: 0.7027\n",
      "Epoch 45/100\n",
      "570/570 [==============================] - 0s - loss: 0.0937 - acc: 0.9667 - val_loss: 0.9336 - val_acc: 0.7162\n",
      "Epoch 46/100\n",
      "570/570 [==============================] - 0s - loss: 0.0749 - acc: 0.9719 - val_loss: 0.9622 - val_acc: 0.7162\n",
      "Epoch 47/100\n",
      "570/570 [==============================] - 0s - loss: 0.0652 - acc: 0.9772 - val_loss: 0.9406 - val_acc: 0.7027\n",
      "Epoch 48/100\n",
      "570/570 [==============================] - 0s - loss: 0.1118 - acc: 0.9579 - val_loss: 1.1010 - val_acc: 0.7297\n",
      "Epoch 49/100\n",
      "570/570 [==============================] - 0s - loss: 0.0725 - acc: 0.9684 - val_loss: 0.8926 - val_acc: 0.7297\n",
      "Epoch 50/100\n",
      "570/570 [==============================] - 0s - loss: 0.0916 - acc: 0.9684 - val_loss: 1.0202 - val_acc: 0.7162\n",
      "Epoch 51/100\n",
      "570/570 [==============================] - 0s - loss: 0.0680 - acc: 0.9737 - val_loss: 1.1211 - val_acc: 0.7162\n",
      "Epoch 52/100\n",
      "570/570 [==============================] - 0s - loss: 0.0717 - acc: 0.9737 - val_loss: 1.1629 - val_acc: 0.6757\n",
      "Epoch 53/100\n",
      "570/570 [==============================] - 0s - loss: 0.0702 - acc: 0.9684 - val_loss: 1.1045 - val_acc: 0.6892\n",
      "Epoch 54/100\n",
      "570/570 [==============================] - 0s - loss: 0.0457 - acc: 0.9860 - val_loss: 1.1658 - val_acc: 0.7568\n",
      "Epoch 55/100\n",
      "570/570 [==============================] - 0s - loss: 0.0701 - acc: 0.9754 - val_loss: 1.2144 - val_acc: 0.7432\n",
      "Epoch 56/100\n",
      "570/570 [==============================] - 0s - loss: 0.0404 - acc: 0.9895 - val_loss: 1.0661 - val_acc: 0.7297\n",
      "Epoch 57/100\n",
      "570/570 [==============================] - 0s - loss: 0.0798 - acc: 0.9702 - val_loss: 1.1390 - val_acc: 0.7027\n",
      "Epoch 58/100\n",
      "570/570 [==============================] - 0s - loss: 0.0749 - acc: 0.9719 - val_loss: 1.1594 - val_acc: 0.7162\n",
      "Epoch 59/100\n",
      "570/570 [==============================] - 0s - loss: 0.0703 - acc: 0.9719 - val_loss: 1.1480 - val_acc: 0.6622\n",
      "Epoch 60/100\n",
      "570/570 [==============================] - 0s - loss: 0.0439 - acc: 0.9877 - val_loss: 1.2836 - val_acc: 0.6622\n",
      "Epoch 61/100\n",
      "570/570 [==============================] - 0s - loss: 0.0672 - acc: 0.9754 - val_loss: 1.2504 - val_acc: 0.6757\n",
      "Epoch 62/100\n",
      "570/570 [==============================] - 0s - loss: 0.0810 - acc: 0.9596 - val_loss: 1.1929 - val_acc: 0.6892\n",
      "Epoch 63/100\n",
      "570/570 [==============================] - 0s - loss: 0.0688 - acc: 0.9754 - val_loss: 1.3936 - val_acc: 0.7027\n",
      "Epoch 64/100\n",
      "570/570 [==============================] - 0s - loss: 0.0543 - acc: 0.9825 - val_loss: 1.3491 - val_acc: 0.7027\n",
      "Epoch 65/100\n",
      "570/570 [==============================] - 0s - loss: 0.0452 - acc: 0.9842 - val_loss: 1.1908 - val_acc: 0.7297\n",
      "Epoch 66/100\n",
      "570/570 [==============================] - 0s - loss: 0.0437 - acc: 0.9807 - val_loss: 1.1096 - val_acc: 0.7162\n",
      "Epoch 67/100\n",
      "570/570 [==============================] - 0s - loss: 0.0781 - acc: 0.9754 - val_loss: 1.0293 - val_acc: 0.7297\n",
      "Epoch 68/100\n",
      "570/570 [==============================] - 0s - loss: 0.0546 - acc: 0.9842 - val_loss: 1.1131 - val_acc: 0.7027\n",
      "Epoch 69/100\n",
      "570/570 [==============================] - 0s - loss: 0.0538 - acc: 0.9789 - val_loss: 1.1660 - val_acc: 0.7162\n",
      "Epoch 70/100\n",
      "570/570 [==============================] - 0s - loss: 0.0429 - acc: 0.9912 - val_loss: 1.3354 - val_acc: 0.6622\n",
      "Epoch 71/100\n",
      "570/570 [==============================] - 0s - loss: 0.0770 - acc: 0.9632 - val_loss: 1.4018 - val_acc: 0.7162\n",
      "Epoch 72/100\n",
      "570/570 [==============================] - 0s - loss: 0.0463 - acc: 0.9789 - val_loss: 1.1304 - val_acc: 0.7162\n",
      "Epoch 73/100\n",
      "570/570 [==============================] - 0s - loss: 0.0540 - acc: 0.9807 - val_loss: 1.2574 - val_acc: 0.7162\n",
      "Epoch 74/100\n",
      "570/570 [==============================] - 0s - loss: 0.0732 - acc: 0.9667 - val_loss: 1.1887 - val_acc: 0.7162\n",
      "Epoch 75/100\n",
      "570/570 [==============================] - 0s - loss: 0.0610 - acc: 0.9772 - val_loss: 1.1415 - val_acc: 0.7297\n",
      "Epoch 76/100\n",
      "570/570 [==============================] - 0s - loss: 0.0439 - acc: 0.9842 - val_loss: 1.2089 - val_acc: 0.7297\n",
      "Epoch 77/100\n",
      "570/570 [==============================] - 0s - loss: 0.0400 - acc: 0.9877 - val_loss: 1.1814 - val_acc: 0.7432\n",
      "Epoch 78/100\n",
      "570/570 [==============================] - 0s - loss: 0.0502 - acc: 0.9825 - val_loss: 1.2380 - val_acc: 0.7568\n",
      "Epoch 79/100\n",
      "570/570 [==============================] - 0s - loss: 0.0462 - acc: 0.9789 - val_loss: 1.3002 - val_acc: 0.7432\n",
      "Epoch 80/100\n",
      "570/570 [==============================] - 0s - loss: 0.0413 - acc: 0.9895 - val_loss: 1.1872 - val_acc: 0.7027\n",
      "Epoch 81/100\n",
      "570/570 [==============================] - 0s - loss: 0.0572 - acc: 0.9772 - val_loss: 1.1827 - val_acc: 0.7162\n",
      "Epoch 82/100\n",
      "570/570 [==============================] - 0s - loss: 0.0646 - acc: 0.9772 - val_loss: 1.1961 - val_acc: 0.7432\n",
      "Epoch 83/100\n",
      "570/570 [==============================] - 0s - loss: 0.0431 - acc: 0.9842 - val_loss: 1.2383 - val_acc: 0.7297\n",
      "Epoch 84/100\n",
      "570/570 [==============================] - 0s - loss: 0.0956 - acc: 0.9702 - val_loss: 1.3838 - val_acc: 0.7432\n",
      "Epoch 85/100\n",
      "570/570 [==============================] - 0s - loss: 0.0457 - acc: 0.9825 - val_loss: 1.1198 - val_acc: 0.7162\n",
      "Epoch 86/100\n",
      "570/570 [==============================] - 0s - loss: 0.0534 - acc: 0.9789 - val_loss: 1.2181 - val_acc: 0.7027\n",
      "Epoch 87/100\n",
      "570/570 [==============================] - 0s - loss: 0.0792 - acc: 0.9632 - val_loss: 1.1540 - val_acc: 0.7297\n",
      "Epoch 88/100\n",
      "570/570 [==============================] - 0s - loss: 0.0588 - acc: 0.9789 - val_loss: 1.2220 - val_acc: 0.7162\n",
      "Epoch 89/100\n",
      "570/570 [==============================] - 0s - loss: 0.0304 - acc: 0.9895 - val_loss: 1.2398 - val_acc: 0.7162\n",
      "Epoch 90/100\n",
      "570/570 [==============================] - 0s - loss: 0.0305 - acc: 0.9860 - val_loss: 1.2573 - val_acc: 0.7432\n",
      "Epoch 91/100\n",
      "570/570 [==============================] - 0s - loss: 0.0473 - acc: 0.9895 - val_loss: 1.2043 - val_acc: 0.7432\n",
      "Epoch 92/100\n",
      "570/570 [==============================] - 0s - loss: 0.0443 - acc: 0.9825 - val_loss: 1.3231 - val_acc: 0.7162\n",
      "Epoch 93/100\n",
      "570/570 [==============================] - 0s - loss: 0.0345 - acc: 0.9912 - val_loss: 1.3252 - val_acc: 0.7027\n",
      "Epoch 94/100\n",
      "570/570 [==============================] - 0s - loss: 0.0969 - acc: 0.9596 - val_loss: 1.5769 - val_acc: 0.7162\n",
      "Epoch 95/100\n",
      "570/570 [==============================] - 0s - loss: 0.0553 - acc: 0.9789 - val_loss: 1.4022 - val_acc: 0.7162\n",
      "Epoch 96/100\n",
      "570/570 [==============================] - 0s - loss: 0.0585 - acc: 0.9807 - val_loss: 1.3072 - val_acc: 0.7297\n",
      "Epoch 97/100\n",
      "570/570 [==============================] - 0s - loss: 0.0784 - acc: 0.9737 - val_loss: 1.2138 - val_acc: 0.7027\n",
      "Epoch 98/100\n",
      "570/570 [==============================] - 0s - loss: 0.0589 - acc: 0.9772 - val_loss: 1.2647 - val_acc: 0.7432\n",
      "Epoch 99/100\n",
      "570/570 [==============================] - 0s - loss: 0.0575 - acc: 0.9737 - val_loss: 0.8892 - val_acc: 0.7432\n",
      "Epoch 100/100\n",
      "570/570 [==============================] - 0s - loss: 0.0425 - acc: 0.9860 - val_loss: 1.0357 - val_acc: 0.7297\n"
     ]
    }
   ],
   "source": [
    "top_model_history = top_model.fit(x=bottleneck_train_data, y=y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    validation_data=(bottleneck_validation_data, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model.save_weights('bottleneck_top_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # New top layers after mobilenet_base_model\n",
    "# # x = Flatten()(mobilenet_base_model.output)\n",
    "# Inp = Input(shape=(1024,))\n",
    "# # x = Dropout(0.2)(mobilenet_base_model.output)\n",
    "# x = Dropout(0.2)(Inp)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dense(1024, activation='relu', name = \"Dense_1\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# predictions = Dense(num_classes, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top_model = Model(input=Inp, output=predictions)\n",
    "# top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top_model.load_weights('bottleneck_top_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model.load_weights('bottleneck_top_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/74 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_model.evaluate(bottleneck_validation_data, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/74 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_model.evaluate(mobilenet_base_model.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whole_model = mobilenet_base_model\n",
    "# whole_model.add(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "whole_model = Model(input=mobilenet_base_model.input, output=top_model(mobilenet_base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 9s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 2)                 1059842   \n",
      "=================================================================\n",
      "Total params: 4,288,706\n",
      "Trainable params: 4,262,722\n",
      "Non-trainable params: 25,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "whole_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.applications.mobilenet.DepthwiseConv2D at 0x7f4438671128>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.layers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "first_trainable_layer : <keras.applications.mobilenet.DepthwiseConv2D object at 0x7f44383f5ba8>\n"
     ]
    }
   ],
   "source": [
    "first_trainable_layer_index = 22\n",
    "print(len(whole_model.layers))\n",
    "print(\"first_trainable_layer :\", whole_model.layers[first_trainable_layer_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f446167b3c8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f446167b668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f446167b5f8>\n",
      "<keras.layers.core.Activation object at 0x7f446167b908>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f446167bd68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44662f0ac8>\n",
      "<keras.layers.core.Activation object at 0x7f443874bcc0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f443874bda0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44386f4320>\n",
      "<keras.layers.core.Activation object at 0x7f4438671e80>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4438671128>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44385fff60>\n",
      "<keras.layers.core.Activation object at 0x7f443869c2e8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4438619a58>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44385c3160>\n",
      "<keras.layers.core.Activation object at 0x7f4438525cf8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4438541e80>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44384e9320>\n",
      "<keras.layers.core.Activation object at 0x7f44384d2b00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4438466710>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44384931d0>\n",
      "<keras.layers.core.Activation object at 0x7f44383f5f98>\n"
     ]
    }
   ],
   "source": [
    "for layer in whole_model.layers[:first_trainable_layer_index]:\n",
    "    print(layer)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f44383f5ba8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44383a3a58>\n",
      "<keras.layers.core.Activation object at 0x7f443839ed30>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f443839ef28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f443834f908>\n",
      "<keras.layers.core.Activation object at 0x7f44382c8c50>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f44382c8860>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4438275710>\n",
      "<keras.layers.core.Activation object at 0x7f44381f0a20>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f44381f09e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f443821d5c0>\n",
      "<keras.layers.core.Activation object at 0x7f4438198dd8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4438198e48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4438141588>\n",
      "<keras.layers.core.Activation object at 0x7f44380c2c18>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f44380c2cf8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4438053f60>\n",
      "<keras.layers.core.Activation object at 0x7f4437febdd8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437feb0b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437f7ceb8>\n",
      "<keras.layers.core.Activation object at 0x7f4438016128>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4437f959b0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437f3f0f0>\n",
      "<keras.layers.core.Activation object at 0x7f4437ea6e48>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437ebedd8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437e6b278>\n",
      "<keras.layers.core.Activation object at 0x7f4437e4ffd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4437de8668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437e13080>\n",
      "<keras.layers.core.Activation object at 0x7f4437d7a9b0>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437d7aef0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437d269b0>\n",
      "<keras.layers.core.Activation object at 0x7f4437d21e80>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4437d21d68>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437ccd860>\n",
      "<keras.layers.core.Activation object at 0x7f4437c4cba8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437c4c668>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437bf7668>\n",
      "<keras.layers.core.Activation object at 0x7f4437b76eb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4437b76f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437ba2518>\n",
      "<keras.layers.core.Activation object at 0x7f4437b1ed30>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437b1eda0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437ac74e0>\n",
      "<keras.layers.core.Activation object at 0x7f4437a4bf28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f4437a4bc50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44379f3390>\n",
      "<keras.layers.core.Activation object at 0x7f443796fd30>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f443796f048>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437904f98>\n",
      "<keras.layers.core.Activation object at 0x7f4437904ef0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f443791c908>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44378c74e0>\n",
      "<keras.layers.core.Activation object at 0x7f443782dda0>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f4437846d30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44377f01d0>\n",
      "<keras.layers.core.Activation object at 0x7f443776b5f8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f44377d5f28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f4437781b00>\n",
      "<keras.layers.core.Activation object at 0x7f44376fe908>\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f44376fee48>\n",
      "<keras.engine.training.Model object at 0x7f44375ea5c0>\n"
     ]
    }
   ],
   "source": [
    "for layer in whole_model.layers[first_trainable_layer_index:]:\n",
    "    print(layer)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an \n",
    "# # adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays \n",
    "# # very small, so as not to wreck the previously learned features\n",
    "\n",
    "# # whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# whole_model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 2)                 1059842   \n",
      "=================================================================\n",
      "Total params: 4,288,706\n",
      "Trainable params: 4,232,066\n",
      "Non-trainable params: 56,640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "whole_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# head_model.fit(x, y, batch_size = batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 2s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = whole_model.fit(x=x_train, y=y_train,\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=training_epochs,\n",
    "#                     verbose=1, # This is for what we want it to display out as it trains \n",
    "#                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(base_lr=0.00005, max_lr=0.0002,\n",
    "                    step_size=1500., mode='triangular2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 570 samples, validate on 74 samples\n",
      "Epoch 1/20\n",
      "570/570 [==============================] - 98s - loss: 2.1059 - acc: 0.5842 - val_loss: 3.3513 - val_acc: 0.6486\n",
      "Epoch 2/20\n",
      "570/570 [==============================] - 77s - loss: 1.8876 - acc: 0.6404 - val_loss: 2.9156 - val_acc: 0.5676\n",
      "Epoch 3/20\n",
      "570/570 [==============================] - 68s - loss: 1.3464 - acc: 0.7123 - val_loss: 4.5297 - val_acc: 0.4730\n",
      "Epoch 4/20\n",
      "570/570 [==============================] - 67s - loss: 1.0584 - acc: 0.7368 - val_loss: 3.4569 - val_acc: 0.6216\n",
      "Epoch 5/20\n",
      "570/570 [==============================] - 78s - loss: 1.0411 - acc: 0.7281 - val_loss: 2.6953 - val_acc: 0.5676\n",
      "Epoch 6/20\n",
      "570/570 [==============================] - 86s - loss: 0.7234 - acc: 0.8018 - val_loss: 1.9633 - val_acc: 0.6622\n",
      "Epoch 7/20\n",
      "570/570 [==============================] - 88s - loss: 0.7158 - acc: 0.7965 - val_loss: 1.6992 - val_acc: 0.6757\n",
      "Epoch 8/20\n",
      "570/570 [==============================] - 89s - loss: 0.7744 - acc: 0.7965 - val_loss: 2.3379 - val_acc: 0.6351\n",
      "Epoch 9/20\n",
      "570/570 [==============================] - 89s - loss: 0.8922 - acc: 0.7526 - val_loss: 5.1694 - val_acc: 0.5541\n",
      "Epoch 10/20\n",
      "570/570 [==============================] - 93s - loss: 0.6842 - acc: 0.7982 - val_loss: 4.2066 - val_acc: 0.5946\n",
      "Epoch 11/20\n",
      "570/570 [==============================] - 88s - loss: 0.7372 - acc: 0.7842 - val_loss: 3.1100 - val_acc: 0.5811\n",
      "Epoch 12/20\n",
      "570/570 [==============================] - 90s - loss: 0.6776 - acc: 0.7842 - val_loss: 2.2538 - val_acc: 0.6622\n",
      "Epoch 13/20\n",
      "570/570 [==============================] - 89s - loss: 0.6691 - acc: 0.8070 - val_loss: 1.7522 - val_acc: 0.6757\n",
      "Epoch 14/20\n",
      "570/570 [==============================] - 88s - loss: 0.7997 - acc: 0.7877 - val_loss: 1.5986 - val_acc: 0.7568\n",
      "Epoch 15/20\n",
      "570/570 [==============================] - 92s - loss: 0.4796 - acc: 0.8544 - val_loss: 1.3834 - val_acc: 0.7432\n",
      "Epoch 16/20\n",
      "570/570 [==============================] - 102s - loss: 0.3998 - acc: 0.8614 - val_loss: 1.2288 - val_acc: 0.7297\n",
      "Epoch 17/20\n",
      "570/570 [==============================] - 94s - loss: 0.4491 - acc: 0.8632 - val_loss: 1.0642 - val_acc: 0.7973\n",
      "Epoch 18/20\n",
      "570/570 [==============================] - 95s - loss: 0.2970 - acc: 0.9053 - val_loss: 1.4822 - val_acc: 0.7027\n",
      "Epoch 19/20\n",
      "570/570 [==============================] - 95s - loss: 0.4842 - acc: 0.8596 - val_loss: 1.3522 - val_acc: 0.7568\n",
      "Epoch 20/20\n",
      "570/570 [==============================] - 91s - loss: 0.7263 - acc: 0.8000 - val_loss: 1.3269 - val_acc: 0.6892\n"
     ]
    }
   ],
   "source": [
    "history = whole_model.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99981999e-01,   1.80099341e-05],\n",
       "       [  9.85866845e-01,   1.41331861e-02],\n",
       "       [  9.99754369e-01,   2.45599338e-04],\n",
       "       [  8.91232491e-01,   1.08767517e-01],\n",
       "       [  7.51295447e-01,   2.48704538e-01]], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.predict_on_batch(x_test[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0831207633018\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = whole_model.evaluate(x_test[10:15], y_test[10:15], verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 3s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3269044884631562, 0.68918918918918914]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_model.save(\"mobilenet_voice_sentiment_model_SGD_1_clr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 3s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3269044884631562, 0.68918918918918914]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'relu6': keras.applications.mobilenet.relu6,'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D}):    \n",
    "    loaded_model = load_model(\"mobilenet_voice_sentiment_model_SGD_1_clr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3269044884631562, 0.68918918918918914]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Training more lower layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-initializing mobilenet_base_model & top_model\n",
    "\n",
    "top_model.load_weights('bottleneck_top_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobilenet_base_model = MobileNet(\n",
    "    input_shape=(224, 224, 3),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "#     pooling=None,\n",
    "    pooling='avg',\n",
    "    classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"mo...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "whole_model_2 = Model(input=mobilenet_base_model.input, output=top_model(mobilenet_base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 2)                 1059842   \n",
      "=================================================================\n",
      "Total params: 4,288,706\n",
      "Trainable params: 4,262,722\n",
      "Non-trainable params: 25,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "whole_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an \n",
    "# adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays \n",
    "# very small, so as not to wreck the previously learned features\n",
    "\n",
    "# whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "whole_model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=1e-4, decay=0.1),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 4s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "first_trainable_layer : <keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ffb01fd0>\n",
      "first_trainable_layer name : conv_dw_4\n"
     ]
    }
   ],
   "source": [
    "first_trainable_layer_index = 22\n",
    "print(len(whole_model_2.layers))\n",
    "print(\"first_trainable_layer :\", whole_model_2.layers[first_trainable_layer_index])\n",
    "print(\"first_trainable_layer name :\", whole_model_2.layers[first_trainable_layer_index].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f44046c94e0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f44046c96d8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f44046c92b0>\n",
      "<keras.layers.core.Activation object at 0x7f44046c9da0>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f44046c9d30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffe1a748>\n",
      "<keras.layers.core.Activation object at 0x7f43ffe46d68>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ffe46e48>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffdec3c8>\n",
      "<keras.layers.core.Activation object at 0x7f43ffd68c50>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ffd68f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffd11400>\n",
      "<keras.layers.core.Activation object at 0x7f43ffc90f28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ffc90c50>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffcb9390>\n",
      "<keras.layers.core.Activation object at 0x7f43ffc34e10>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ffc340b8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffbc4fd0>\n",
      "<keras.layers.core.Activation object at 0x7f43ffbe0278>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ffb5cac8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffb850f0>\n",
      "<keras.layers.core.Activation object at 0x7f43ffaebfd0>\n"
     ]
    }
   ],
   "source": [
    "for layer in whole_model_2.layers[:first_trainable_layer_index]:\n",
    "    print(layer)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ffb01fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ffa14cf8>\n",
      "<keras.layers.core.Activation object at 0x7f43ffaad0f0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ffa2c940>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff9d5518>\n",
      "<keras.layers.core.Activation object at 0x7f43ff9b9e80>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff955e10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff97f2b0>\n",
      "<keras.layers.core.Activation object at 0x7f43ff8e1c18>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff8f9780>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff8a5240>\n",
      "<keras.layers.core.Activation object at 0x7f43ff820c88>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff888ba8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff7cd080>\n",
      "<keras.layers.core.Activation object at 0x7f43ff7c8630>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff7b0f60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff75bb38>\n",
      "<keras.layers.core.Activation object at 0x7f43ff6d6a20>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff6d6f60>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff705a20>\n",
      "<keras.layers.core.Activation object at 0x7f43ff67ddd8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff67dfd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff62c9b0>\n",
      "<keras.layers.core.Activation object at 0x7f43ff5a7dd8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff5a79e8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff555898>\n",
      "<keras.layers.core.Activation object at 0x7f43ff4cfe48>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff4cfd30>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff4fe828>\n",
      "<keras.layers.core.Activation object at 0x7f43ff477c50>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff477860>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff424710>\n",
      "<keras.layers.core.Activation object at 0x7f43ff3a1cc0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff3a1ba8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff3ca6a0>\n",
      "<keras.layers.core.Activation object at 0x7f43ff347588>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff347f28>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff2f4588>\n",
      "<keras.layers.core.Activation object at 0x7f43ff270eb8>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff270f98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff21c518>\n",
      "<keras.layers.core.Activation object at 0x7f43ff196e10>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff196e80>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff1c15c0>\n",
      "<keras.layers.core.Activation object at 0x7f43ff141d30>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43ff141e10>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff0eb390>\n",
      "<keras.layers.core.Activation object at 0x7f43ff069cf8>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43ff069fd0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43ff011438>\n",
      "<keras.layers.core.Activation object at 0x7f43fef92fd0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43fef92c88>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43fefba3c8>\n",
      "<keras.layers.core.Activation object at 0x7f43fef3be48>\n",
      "<keras.applications.mobilenet.DepthwiseConv2D object at 0x7f43fef3b0f0>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43fee4cfd0>\n",
      "<keras.layers.core.Activation object at 0x7f43feee32b0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f43fee60b00>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f43fee0e128>\n",
      "<keras.layers.core.Activation object at 0x7f43fedf2f28>\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f43fee0b358>\n",
      "<keras.engine.training.Model object at 0x7f44375ea5c0>\n"
     ]
    }
   ],
   "source": [
    "for layer in whole_model_2.layers[first_trainable_layer_index:]:\n",
    "    print(layer)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0356611453399465, 0.72972972972972971]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "model_5 (Model)              (None, 2)                 1059842   \n",
      "=================================================================\n",
      "Total params: 4,288,706\n",
      "Trainable params: 4,232,066\n",
      "Non-trainable params: 56,640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "whole_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 570 samples, validate on 74 samples\n",
      "Epoch 1/20\n",
      " 32/570 [>.............................] - ETA: 776s - loss: 1.8554 - acc: 0.6250 "
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1024,1024]\n\t [[Node: training_10/Adam/Square_83 = Square[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_10/Adam/gradients/model_5_2/Dense_1/MatMul_grad/MatMul_1)]]\n\nCaused by op 'training_10/Adam/Square_83', defined at:\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-172-4fb7b84081d6>\", line 6, in <module>\n    validation_data=(x_test, y_test))\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\", line 1575, in fit\n    self._make_train_function()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\", line 960, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/optimizers.py\", line 433, in get_updates\n    v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1358, in square\n    return tf.square(x)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 447, in square\n    return gen_math_ops.square(x, name=name)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2591, in square\n    result = _op_def_lib.apply_op(\"Square\", x=x, name=name)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,1024]\n\t [[Node: training_10/Adam/Square_83 = Square[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_10/Adam/gradients/model_5_2/Dense_1/MatMul_grad/MatMul_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,1024]\n\t [[Node: training_10/Adam/Square_83 = Square[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_10/Adam/gradients/model_5_2/Dense_1/MatMul_grad/MatMul_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-4fb7b84081d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# This is for what we want it to display out as it trains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1024,1024]\n\t [[Node: training_10/Adam/Square_83 = Square[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_10/Adam/gradients/model_5_2/Dense_1/MatMul_grad/MatMul_1)]]\n\nCaused by op 'training_10/Adam/Square_83', defined at:\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-172-4fb7b84081d6>\", line 6, in <module>\n    validation_data=(x_test, y_test))\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\", line 1575, in fit\n    self._make_train_function()\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/engine/training.py\", line 960, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/optimizers.py\", line 433, in get_updates\n    v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 1358, in square\n    return tf.square(x)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 447, in square\n    return gen_math_ops.square(x, name=name)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2591, in square\n    result = _op_def_lib.apply_op(\"Square\", x=x, name=name)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/cheeseprata/anaconda3/envs/TF13-PY3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,1024]\n\t [[Node: training_10/Adam/Square_83 = Square[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_10/Adam/gradients/model_5_2/Dense_1/MatMul_grad/MatMul_1)]]\n"
     ]
    }
   ],
   "source": [
    "history = whole_model_2.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = whole_model_2.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_model_2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_model_2.save(\"mobilenet_voice_sentiment_model_3_clr.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "\n",
    "with CustomObjectScope({'relu6': keras.applications.mobilenet.relu6,'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D}):    \n",
    "    loaded_model = load_model(\"mobilenet_voice_sentiment_model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-initializing mobilenet_base_model & top_model\n",
    "\n",
    "top_model.load_weights('bottleneck_top_model.h5')\n",
    "\n",
    "mobilenet_base_model = MobileNet(\n",
    "    input_shape=(224, 224, 3),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "#     pooling=None,\n",
    "    pooling='avg',\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "whole_model_3 = Model(input=mobilenet_base_model.input, output=top_model(mobilenet_base_model.output))\n",
    "\n",
    "whole_model_3.summary()\n",
    "\n",
    "# fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an \n",
    "# adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays \n",
    "# very small, so as not to wreck the previously learned features\n",
    "\n",
    "# whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "whole_model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=1e-5, decay=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "whole_model_3.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_trainable_layer_index = 22\n",
    "print(len(whole_model_3.layers))\n",
    "print(\"first_trainable_layer :\", whole_model_3.layers[first_trainable_layer_index])\n",
    "print(\"first_trainable_layer name :\", whole_model_3.layers[first_trainable_layer_index].name)\n",
    "\n",
    "for layer in whole_model_3.layers[:first_trainable_layer_index]:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in whole_model_3.layers[first_trainable_layer_index:]:\n",
    "    print(layer)\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = whole_model_3.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-initializing mobilenet_base_model & top_model\n",
    "\n",
    "top_model.load_weights('bottleneck_top_model.h5')\n",
    "\n",
    "mobilenet_base_model = MobileNet(\n",
    "    input_shape=(224, 224, 3),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "#     pooling=None,\n",
    "    pooling='avg',\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "whole_model_4 = Model(input=mobilenet_base_model.input, output=top_model(mobilenet_base_model.output))\n",
    "\n",
    "whole_model_4.summary()\n",
    "\n",
    "# fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an \n",
    "# adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays \n",
    "# very small, so as not to wreck the previously learned features\n",
    "\n",
    "# whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "whole_model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=1e-4, decay=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "whole_model_4.evaluate(x_test, y_test)\n",
    "\n",
    "first_trainable_layer_index = 34\n",
    "print(len(whole_model_4.layers))\n",
    "print(\"first_trainable_layer :\", whole_model_4.layers[first_trainable_layer_index])\n",
    "print(\"first_trainable_layer name :\", whole_model_4.layers[first_trainable_layer_index].name)\n",
    "\n",
    "for layer in whole_model_4.layers[:first_trainable_layer_index]:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in whole_model_4.layers[first_trainable_layer_index:]:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "\n",
    "history = whole_model_4.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-initializing mobilenet_base_model & top_model\n",
    "\n",
    "top_model.load_weights('bottleneck_top_model.h5')\n",
    "\n",
    "mobilenet_base_model = MobileNet(\n",
    "    input_shape=(224, 224, 3),\n",
    "    alpha=1.0,\n",
    "    depth_multiplier=1,\n",
    "    dropout=0.001,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "#     pooling=None,\n",
    "    pooling='avg',\n",
    "    classes=2\n",
    ")\n",
    "\n",
    "whole_model_5 = Model(input=mobilenet_base_model.input, output=top_model(mobilenet_base_model.output))\n",
    "\n",
    "whole_model_5.summary()\n",
    "\n",
    "# fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an \n",
    "# adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays \n",
    "# very small, so as not to wreck the previously learned features\n",
    "\n",
    "# whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "whole_model_5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=1e-5, decay=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "whole_model_5.evaluate(x_test, y_test)\n",
    "\n",
    "first_trainable_layer_index = 34\n",
    "print(len(whole_model_5.layers))\n",
    "print(\"first_trainable_layer :\", whole_model_5.layers[first_trainable_layer_index])\n",
    "print(\"first_trainable_layer name :\", whole_model_5.layers[first_trainable_layer_index].name)\n",
    "\n",
    "for layer in whole_model_5.layers[:first_trainable_layer_index]:\n",
    "    print(layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in whole_model_5.layers[first_trainable_layer_index:]:\n",
    "    print(layer)\n",
    "    layer.trainable = True\n",
    "\n",
    "history = whole_model_5.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = whole_model_5.fit(x=x_train, y=y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1, # This is for what we want it to display out as it trains \n",
    "                    callbacks=[clr],\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_model_5.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
